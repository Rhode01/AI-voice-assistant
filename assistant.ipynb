{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "71ae48f0-c58d-43e6-93ba-7e6497d0e917",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "from transformers import DetrForObjectDetection, DetrImageProcessor, pipeline\n",
    "import torchvision.transforms as transforms\n",
    "import cv2 as cv\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq\n",
    "import gradio as gr \n",
    "import requests\n",
    "import numpy as np\n",
    "from transformers import BlenderbotTokenizer, BlenderbotForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e4cff16d-ec8c-400b-8fc8-10097b0de552",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./models/object-detection-resnet-50\"\n",
    "processor =  DetrImageProcessor.from_pretrained(model_path)\n",
    "model = DetrForObjectDetection.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffa51d6-4215-436c-9fce-02ebb2af9bdf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "Could not create share link. Please check your internet connection or our status page: https://status.gradio.app.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cap = cv.VideoCapture(0)\n",
    "while True:    \n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    frame_tensor = torch.as_tensor(frame, dtype=torch.float32)\n",
    "    image_frame = processor(images=frame_tensor, return_tensors=\"pt\")\n",
    "    outputs= model(**image_frame)\n",
    "    \n",
    "    target_sizes = torch.tensor([[frame_tensor.shape[1], frame_tensor.shape[2]]])\n",
    "\n",
    "    detections = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.9)[0]\n",
    "    for score, label, box in zip(detections[\"scores\"], detections[\"labels\"], detections[\"boxes\"]):\n",
    "        box = [int(i) for i in box.tolist()]\n",
    "        cv.rectangle(frame, (box[0], box[1]), (box[2], box[3]), (0, 255, 0), 2) \n",
    "        label_text = f\"{model.config.id2label[label.item()]}: {round(score.item(), 2)}\"\n",
    "        cv.putText(frame, label_text, (box[0], box[1] - 10), cv.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "    if label_text.startswith (\"person\"):\n",
    "        start_conversation()\n",
    "        break\n",
    "    cv.imshow(\"frame\", frame)\n",
    "    if cv.waitKey(1) == ord('q'):\n",
    "         break\n",
    "cap.release()\n",
    "cv.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7610140-9310-4a4e-bbf1-8d67ab76ac39",
   "metadata": {},
   "source": [
    "#### creating a conversation with someone\n",
    "Reading the input from the microphone\n",
    "detect the spoken langauge \n",
    "convert the microphone sampling rate to 16kz\n",
    "convert the spoken words to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7cfcc84f-4e05-4222-928c-b2f4d7587781",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def start_conversation():\n",
    "    whisper_model_path= \"./models/automatic-speech-recognition\"\n",
    "    processor = AutoProcessor.from_pretrained(whisper_model_path)\n",
    "    model = AutoModelForSpeechSeq2Seq.from_pretrained(whisper_model_path, use_safetensors=True)\n",
    "    output= \"\"\n",
    "    torch_dtype = torch.float32\n",
    "\n",
    "    whisper_pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    max_new_tokens=128,\n",
    "    chunk_length_s=30,\n",
    "    batch_size=16,\n",
    "    torch_dtype=torch_dtype\n",
    "    )\n",
    "\n",
    "    demo = gr.Blocks()\n",
    "\n",
    "    def transcribe_speech(file_path):\n",
    "        if file_path is None:\n",
    "            gr.Warning(\"No audio found please try again\")\n",
    "            return \"\"\n",
    "        output = whisper_pipe(file_path)\n",
    "        creating_conversation_with_blenderbot(output[\"text\"])\n",
    "        return output[\"text\"]\n",
    "        \n",
    "    mic_transcribe = gr.Interface(\n",
    "    fn =transcribe_speech,\n",
    "    inputs = gr.Audio(sources=\"microphone\",\n",
    "                      type=\"filepath\"),\n",
    "    outputs = gr.Textbox(label=\"Transcription_results\",\n",
    "                         lines=3),\n",
    "    allow_flagging=\"never\")\n",
    "    \n",
    "    def launch_demo():\n",
    "        with demo:\n",
    "            gr.TabbedInterface(\n",
    "                [mic_transcribe],\n",
    "                [\"Transcribe Microphone\"]\n",
    "            )\n",
    "            demo.launch(debug=True, share=True)\n",
    "    launch_demo()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3967d1bd-de61-4cd5-895a-fc97cff115eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "Could not create share link. Please check your internet connection or our status page: https://status.gradio.app.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Design Dep\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\uvicorn\\protocols\\http\\httptools_impl.py\", line 411, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Design Dep\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\uvicorn\\middleware\\proxy_headers.py\", line 69, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Design Dep\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\fastapi\\applications.py\", line 1054, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"C:\\Users\\Design Dep\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\starlette\\applications.py\", line 123, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"C:\\Users\\Design Dep\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\starlette\\middleware\\errors.py\", line 186, in __call__\n",
      "    raise exc\n",
      "  File \"C:\\Users\\Design Dep\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\starlette\\middleware\\errors.py\", line 164, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"C:\\Users\\Design Dep\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\gradio\\route_utils.py\", line 689, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"C:\\Users\\Design Dep\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\starlette\\middleware\\exceptions.py\", line 65, in __call__\n",
      "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  File \"C:\\Users\\Design Dep\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 64, in wrapped_app\n",
      "    raise exc\n",
      "  File \"C:\\Users\\Design Dep\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"C:\\Users\\Design Dep\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\starlette\\routing.py\", line 756, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"C:\\Users\\Design Dep\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\starlette\\routing.py\", line 776, in app\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"C:\\Users\\Design Dep\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\starlette\\routing.py\", line 297, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"C:\\Users\\Design Dep\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\starlette\\routing.py\", line 77, in app\n",
      "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  File \"C:\\Users\\Design Dep\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 64, in wrapped_app\n",
      "    raise exc\n",
      "  File \"C:\\Users\\Design Dep\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"C:\\Users\\Design Dep\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\starlette\\routing.py\", line 75, in app\n",
      "    await response(scope, receive, send)\n",
      "  File \"C:\\Users\\Design Dep\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\starlette\\responses.py\", line 352, in __call__\n",
      "    await send(\n",
      "  File \"C:\\Users\\Design Dep\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 50, in sender\n",
      "    await send(message)\n",
      "  File \"C:\\Users\\Design Dep\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 50, in sender\n",
      "    await send(message)\n",
      "  File \"C:\\Users\\Design Dep\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\starlette\\middleware\\errors.py\", line 161, in _send\n",
      "    await send(message)\n",
      "  File \"C:\\Users\\Design Dep\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\uvicorn\\protocols\\http\\httptools_impl.py\", line 549, in send\n",
      "    raise RuntimeError(\"Response content shorter than Content-Length\")\n",
      "RuntimeError: Response content shorter than Content-Length\n",
      "ERROR:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Design Dep\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\uvicorn\\protocols\\http\\httptools_impl.py\", line 411, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Design Dep\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\uvicorn\\middleware\\proxy_headers.py\", line 69, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Design Dep\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\fastapi\\applications.py\", line 1054, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"C:\\Users\\Design Dep\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\starlette\\applications.py\", line 123, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"C:\\Users\\Design Dep\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\starlette\\middleware\\errors.py\", line 186, in __call__\n",
      "    raise exc\n",
      "  File \"C:\\Users\\Design Dep\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\starlette\\middleware\\errors.py\", line 164, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"C:\\Users\\Design Dep\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\gradio\\route_utils.py\", line 689, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"C:\\Users\\Design Dep\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\starlette\\middleware\\exceptions.py\", line 65, in __call__\n",
      "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  File \"C:\\Users\\Design Dep\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 64, in wrapped_app\n",
      "    raise exc\n",
      "  File \"C:\\Users\\Design Dep\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"C:\\Users\\Design Dep\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\starlette\\routing.py\", line 756, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"C:\\Users\\Design Dep\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\starlette\\routing.py\", line 776, in app\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"C:\\Users\\Design Dep\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\starlette\\routing.py\", line 297, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"C:\\Users\\Design Dep\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\starlette\\routing.py\", line 77, in app\n",
      "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  File \"C:\\Users\\Design Dep\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 64, in wrapped_app\n",
      "    raise exc\n",
      "  File \"C:\\Users\\Design Dep\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"C:\\Users\\Design Dep\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\starlette\\routing.py\", line 75, in app\n",
      "    await response(scope, receive, send)\n",
      "  File \"C:\\Users\\Design Dep\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\starlette\\responses.py\", line 352, in __call__\n",
      "    await send(\n",
      "  File \"C:\\Users\\Design Dep\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 50, in sender\n",
      "    await send(message)\n",
      "  File \"C:\\Users\\Design Dep\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 50, in sender\n",
      "    await send(message)\n",
      "  File \"C:\\Users\\Design Dep\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\starlette\\middleware\\errors.py\", line 161, in _send\n",
      "    await send(message)\n",
      "  File \"C:\\Users\\Design Dep\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\uvicorn\\protocols\\http\\httptools_impl.py\", line 549, in send\n",
      "    raise RuntimeError(\"Response content shorter than Content-Length\")\n",
      "RuntimeError: Response content shorter than Content-Length\n",
      "ERROR:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Design Dep\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\uvicorn\\protocols\\http\\httptools_impl.py\", line 411, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Design Dep\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\uvicorn\\middleware\\proxy_headers.py\", line 69, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Design Dep\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\fastapi\\applications.py\", line 1054, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"C:\\Users\\Design Dep\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\starlette\\applications.py\", line 123, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"C:\\Users\\Design Dep\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\starlette\\middleware\\errors.py\", line 186, in __call__\n",
      "    raise exc\n",
      "  File \"C:\\Users\\Design Dep\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\starlette\\middleware\\errors.py\", line 164, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"C:\\Users\\Design Dep\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\gradio\\route_utils.py\", line 689, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"C:\\Users\\Design Dep\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\starlette\\middleware\\exceptions.py\", line 65, in __call__\n",
      "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  File \"C:\\Users\\Design Dep\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 64, in wrapped_app\n",
      "    raise exc\n",
      "  File \"C:\\Users\\Design Dep\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"C:\\Users\\Design Dep\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\starlette\\routing.py\", line 756, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"C:\\Users\\Design Dep\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\starlette\\routing.py\", line 776, in app\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"C:\\Users\\Design Dep\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\starlette\\routing.py\", line 297, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"C:\\Users\\Design Dep\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\starlette\\routing.py\", line 77, in app\n",
      "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  File \"C:\\Users\\Design Dep\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 64, in wrapped_app\n",
      "    raise exc\n",
      "  File \"C:\\Users\\Design Dep\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"C:\\Users\\Design Dep\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\starlette\\routing.py\", line 75, in app\n",
      "    await response(scope, receive, send)\n",
      "  File \"C:\\Users\\Design Dep\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\starlette\\responses.py\", line 352, in __call__\n",
      "    await send(\n",
      "  File \"C:\\Users\\Design Dep\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 50, in sender\n",
      "    await send(message)\n",
      "  File \"C:\\Users\\Design Dep\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 50, in sender\n",
      "    await send(message)\n",
      "  File \"C:\\Users\\Design Dep\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\starlette\\middleware\\errors.py\", line 161, in _send\n",
      "    await send(message)\n",
      "  File \"C:\\Users\\Design Dep\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\uvicorn\\protocols\\http\\httptools_impl.py\", line 549, in send\n",
      "    raise RuntimeError(\"Response content shorter than Content-Length\")\n",
      "RuntimeError: Response content shorter than Content-Length\n",
      "ERROR:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Design Dep\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\uvicorn\\protocols\\http\\httptools_impl.py\", line 411, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Design Dep\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\uvicorn\\middleware\\proxy_headers.py\", line 69, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Design Dep\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\fastapi\\applications.py\", line 1054, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"C:\\Users\\Design Dep\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\starlette\\applications.py\", line 123, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"C:\\Users\\Design Dep\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\starlette\\middleware\\errors.py\", line 186, in __call__\n",
      "    raise exc\n",
      "  File \"C:\\Users\\Design Dep\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\starlette\\middleware\\errors.py\", line 164, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"C:\\Users\\Design Dep\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\gradio\\route_utils.py\", line 689, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"C:\\Users\\Design Dep\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\starlette\\middleware\\exceptions.py\", line 65, in __call__\n",
      "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  File \"C:\\Users\\Design Dep\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 64, in wrapped_app\n",
      "    raise exc\n",
      "  File \"C:\\Users\\Design Dep\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"C:\\Users\\Design Dep\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\starlette\\routing.py\", line 756, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"C:\\Users\\Design Dep\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\starlette\\routing.py\", line 776, in app\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"C:\\Users\\Design Dep\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\starlette\\routing.py\", line 297, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"C:\\Users\\Design Dep\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\starlette\\routing.py\", line 77, in app\n",
      "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  File \"C:\\Users\\Design Dep\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 64, in wrapped_app\n",
      "    raise exc\n",
      "  File \"C:\\Users\\Design Dep\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"C:\\Users\\Design Dep\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\starlette\\routing.py\", line 75, in app\n",
      "    await response(scope, receive, send)\n",
      "  File \"C:\\Users\\Design Dep\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\starlette\\responses.py\", line 352, in __call__\n",
      "    await send(\n",
      "  File \"C:\\Users\\Design Dep\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 50, in sender\n",
      "    await send(message)\n",
      "  File \"C:\\Users\\Design Dep\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 50, in sender\n",
      "    await send(message)\n",
      "  File \"C:\\Users\\Design Dep\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\starlette\\middleware\\errors.py\", line 161, in _send\n",
      "    await send(message)\n",
      "  File \"C:\\Users\\Design Dep\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\uvicorn\\protocols\\http\\httptools_impl.py\", line 549, in send\n",
      "    raise RuntimeError(\"Response content shorter than Content-Length\")\n",
      "RuntimeError: Response content shorter than Content-Length\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    }
   ],
   "source": [
    "start_conversation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254486de-2256-4e27-9a92-c05dfda1a1ce",
   "metadata": {},
   "source": [
    "##### creating conversation context using blenderbot model\n",
    "from the transcribed spoken words create a conversation chat with\n",
    "create a context to follow up the text messages converted and spoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c6b8da74-faff-41f8-a215-66816416af9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def creating_conversation_with_blenderbot(transcribed_text, conversation_history=[]):\n",
    "    from transformers import Conversation, BlenderbotTokenizer, BlenderbotForConditionalGeneration\n",
    "    blender_model_path = \"./models/facebook\"\n",
    "    \n",
    "    tokenizer = BlenderbotTokenizer.from_pretrained(blender_model_path)\n",
    "    model = BlenderbotForConditionalGeneration.from_pretrained(blender_model_path)\n",
    "\n",
    "    if conversation_history:\n",
    "        transcribed_text = '\\n'.join(conversation_history + [transcribed_text])\n",
    "\n",
    "    inputs = tokenizer([transcribed_text], return_tensors=\"pt\", max_length=128, truncation=True)\n",
    "\n",
    "    reply_ids = model.generate(**inputs)\n",
    "    \n",
    "    response = tokenizer.batch_decode(reply_ids, skip_special_tokens=True)[0]\n",
    "    conversation_history.append(transcribed_text)\n",
    "    conversation_history.append(response)\n",
    "\n",
    "    produce_reply_sound(response)\n",
    "    return conversation_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3d7981c-deba-489a-9634-5d70130fb22e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i love doing NLP do u know anything?',\n",
       " \" I don't know much about it, but I do know that it's a non-profit organization.\",\n",
       " \"i love doing NLP do u know anything?\\n I don't know much about it, but I do know that it's a non-profit organization.\\nNope i meant natural language processing\",\n",
       " ' National Language Academy of the United States is a nonprofit organization that aims to educate and develop native languages.',\n",
       " \"i love doing NLP do u know anything?\\n I don't know much about it, but I do know that it's a non-profit organization.\\ni love doing NLP do u know anything?\\n I don't know much about it, but I do know that it's a non-profit organization.\\nNope i meant natural language processing\\n National Language Academy of the United States is a nonprofit organization that aims to educate and develop native languages.\\noh really i didnt know about that then? so what do know about financial crisis\",\n",
       " ' Financial crisis is a serious issue that affects a lot of people globally.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07bb4715-4828-4030-9f79-5479c8dc29e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_reply_sound(text):\n",
    "    import pyttsx3\n",
    "    engine = pyttsx3.init()\n",
    "    rate = engine.getProperty('rate')\n",
    "    engine.setProperty('rate', rate- 50)\n",
    "    engine.say(text)\n",
    "    engine.runAndWait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0403eb6c-5020-40cd-b429-70b5d4cee31c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
