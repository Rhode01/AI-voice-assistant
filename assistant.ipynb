{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71ae48f0-c58d-43e6-93ba-7e6497d0e917",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "from transformers import DetrForObjectDetection, DetrImageProcessor, pipeline\n",
    "import torchvision.transforms as transforms\n",
    "import cv2 as cv\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq\n",
    "import gradio as gr \n",
    "import requests\n",
    "import numpy as np\n",
    "from transformers import BlenderbotTokenizer, BlenderbotForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4cff16d-ec8c-400b-8fc8-10097b0de552",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./models/object-detection-resnet-50\"\n",
    "processor =  DetrImageProcessor.from_pretrained(model_path)\n",
    "model = DetrForObjectDetection.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ffa51d6-4215-436c-9fce-02ebb2af9bdf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'start_conversation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m     cv\u001b[38;5;241m.\u001b[39mputText(frame, label_text, (box[\u001b[38;5;241m0\u001b[39m], box[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m10\u001b[39m), cv\u001b[38;5;241m.\u001b[39mFONT_HERSHEY_SIMPLEX, \u001b[38;5;241m0.5\u001b[39m, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m label_text\u001b[38;5;241m.\u001b[39mstartswith (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mperson\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m---> 19\u001b[0m     \u001b[43mstart_conversation\u001b[49m()\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     21\u001b[0m cv\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframe\u001b[39m\u001b[38;5;124m\"\u001b[39m, frame)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'start_conversation' is not defined"
     ]
    }
   ],
   "source": [
    "cap = cv.VideoCapture(0)\n",
    "while True:    \n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    frame_tensor = torch.as_tensor(frame, dtype=torch.float32)\n",
    "    image_frame = processor(images=frame_tensor, return_tensors=\"pt\")\n",
    "    outputs= model(**image_frame)\n",
    "    \n",
    "    target_sizes = torch.tensor([[frame_tensor.shape[1], frame_tensor.shape[2]]])\n",
    "\n",
    "    detections = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.9)[0]\n",
    "    for score, label, box in zip(detections[\"scores\"], detections[\"labels\"], detections[\"boxes\"]):\n",
    "        box = [int(i) for i in box.tolist()]\n",
    "        cv.rectangle(frame, (box[0], box[1]), (box[2], box[3]), (0, 255, 0), 2) \n",
    "        label_text = f\"{model.config.id2label[label.item()]}: {round(score.item(), 2)}\"\n",
    "        cv.putText(frame, label_text, (box[0], box[1] - 10), cv.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "    if label_text.startswith (\"person\"):\n",
    "        start_conversation()\n",
    "        break\n",
    "    cv.imshow(\"frame\", frame)\n",
    "    if cv.waitKey(1) == ord('q'):\n",
    "         break\n",
    "cap.release()\n",
    "cv.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7610140-9310-4a4e-bbf1-8d67ab76ac39",
   "metadata": {},
   "source": [
    "#### creating a conversation with someone\n",
    "Reading the input from the microphone\n",
    "detect the spoken langauge \n",
    "convert the microphone sampling rate to 16kz\n",
    "convert the spoken words to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7cfcc84f-4e05-4222-928c-b2f4d7587781",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "def start_conversation():\n",
    "    transcribed_text = \"\"\n",
    "    \n",
    "    whisper_model_path= \"./models/automatic-speech-recognition\"\n",
    "    processor = AutoProcessor.from_pretrained(whisper_model_path)\n",
    "    model = AutoModelForSpeechSeq2Seq.from_pretrained(whisper_model_path, use_safetensors=True)\n",
    "\n",
    "    torch_dtype = torch.float32\n",
    "\n",
    "    whisper_pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    max_new_tokens=128,\n",
    "    chunk_length_s=15,\n",
    "    batch_size=16,\n",
    "    torch_dtype=torch_dtype\n",
    "    )\n",
    "    demo = gr.Blocks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "513b90be-ef83-45a2-a8cf-8b4a7b7bf79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_speech(file_path):\n",
    "    if file_path is None:\n",
    "        gr.Warning(\"No audio found please try again\")\n",
    "        return \"\"\n",
    "    output = whisper_pipe(file_path)\n",
    "    transcribed_text = output[text]\n",
    "    return output[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc90ed80-e7c0-4a2a-96bb-3af6c0e1493c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mic_transcribe = gr.Interface(\n",
    "    fn =transcribe_speech,\n",
    "    inputs = gr.Audio(sources=\"microphone\",\n",
    "                      type=\"filepath\"),\n",
    "    outputs = gr.Textbox(label=\"Transcription\",\n",
    "                         lines=3),\n",
    "    allow_flagging=\"never\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e48d4107-41e9-4f17-8cf6-8e9cce7eb880",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "DuplicateBlockError",
     "evalue": "At least one block in this Blocks has already been rendered.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDuplicateBlockError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m demo:\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTabbedInterface\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mmic_transcribe\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTranscribe Microphone\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     demo\u001b[38;5;241m.\u001b[39mlaunch(debug\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, share\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\gradio\\interface.py:940\u001b[0m, in \u001b[0;36mTabbedInterface.__init__\u001b[1;34m(self, interface_list, tab_names, title, theme, analytics_enabled, css, js, head)\u001b[0m\n\u001b[0;32m    938\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m interface, tab_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(interface_list, tab_names):\n\u001b[0;32m    939\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Tab(label\u001b[38;5;241m=\u001b[39mtab_name):\n\u001b[1;32m--> 940\u001b[0m         \u001b[43minterface\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\gradio\\blocks.py:1167\u001b[0m, in \u001b[0;36mBlocks.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1164\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mid\u001b[39m \u001b[38;5;129;01min\u001b[39;00m overlapping_ids:\n\u001b[0;32m   1165\u001b[0m     \u001b[38;5;66;03m# State components are allowed to be reused between Blocks\u001b[39;00m\n\u001b[0;32m   1166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks[\u001b[38;5;28mid\u001b[39m], components\u001b[38;5;241m.\u001b[39mState):\n\u001b[1;32m-> 1167\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m DuplicateBlockError(\n\u001b[0;32m   1168\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAt least one block in this Blocks has already been rendered.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1169\u001b[0m         )\n\u001b[0;32m   1171\u001b[0m Context\u001b[38;5;241m.\u001b[39mroot_block\u001b[38;5;241m.\u001b[39mblocks\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks)\n\u001b[0;32m   1172\u001b[0m Context\u001b[38;5;241m.\u001b[39mroot_block\u001b[38;5;241m.\u001b[39mfns\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfns)\n",
      "\u001b[1;31mDuplicateBlockError\u001b[0m: At least one block in this Blocks has already been rendered."
     ]
    }
   ],
   "source": [
    "with demo:\n",
    "    gr.TabbedInterface(\n",
    "        [mic_transcribe],\n",
    "        [\"Transcribe Microphone\"]\n",
    "    )\n",
    "    demo.launch(debug=True, share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d840c3-4727-47b7-99c8-4209cb2b3bd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "254486de-2256-4e27-9a92-c05dfda1a1ce",
   "metadata": {},
   "source": [
    "##### creating conversation context using blenderbot model\n",
    "from the transcribed spoken words create a conversation chat with\n",
    "create a context to follow up the text messages converted and spoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cd73c5-d8a4-43ec-b549-22846a9b4292",
   "metadata": {},
   "outputs": [],
   "source": [
    "def creating_conversation_with_blenderbot(transcribed_text):\n",
    "    from transformers import Conversation\n",
    "    blender_model_path = \"./models/facebook\"\n",
    "    tokenizer = BlenderbotTokenizer.from_pretrained(blender_model_path)\n",
    "    model = BlenderbotForConditionalGeneration.from_pretrained(blender_model_path)\n",
    "    \n",
    "    blender_bot_pipeline = pipeline(task=\"conversational\", model =model)\n",
    "    \n",
    "    conversation = Conversation(transcribed_text)\n",
    "    conversation = blender_bot_pipeline(conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bb4715-4828-4030-9f79-5479c8dc29e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_reply_sound(text):\n",
    "    import pyttsx3\n",
    "    engine = pyttsx3.init()\n",
    "    rate = engine.getProperty('rate')\n",
    "    engine.setProperty('rate', rate- 50)\n",
    "    engine.say(text)\n",
    "    engine.runAndWait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0403eb6c-5020-40cd-b429-70b5d4cee31c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Impossible to guess which tokenizer to use. Please provide a PreTrainedTokenizer class or a path/identifier to a pretrained tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m BlenderbotTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(blender_model_path)\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m BlenderbotForConditionalGeneration\u001b[38;5;241m.\u001b[39mfrom_pretrained(blender_model_path)\n\u001b[1;32m----> 5\u001b[0m blender_bot_pipeline \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconversational\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m user_message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124mHello my name is\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     10\u001b[0m conversation \u001b[38;5;241m=\u001b[39m Conversation(user_message)\n",
      "File \u001b[1;32m~\\Desktop\\Rhodrick\\enviro\\Lib\\site-packages\\transformers\\pipelines\\__init__.py:988\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    985\u001b[0m         tokenizer \u001b[38;5;241m=\u001b[39m config\n\u001b[0;32m    986\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    987\u001b[0m         \u001b[38;5;66;03m# Impossible to guess what is the right tokenizer here\u001b[39;00m\n\u001b[1;32m--> 988\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[0;32m    989\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImpossible to guess which tokenizer to use. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    990\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease provide a PreTrainedTokenizer class or a path/identifier to a pretrained tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    991\u001b[0m         )\n\u001b[0;32m    993\u001b[0m \u001b[38;5;66;03m# Instantiate tokenizer if needed\u001b[39;00m\n\u001b[0;32m    994\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tokenizer, (\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n",
      "\u001b[1;31mException\u001b[0m: Impossible to guess which tokenizer to use. Please provide a PreTrainedTokenizer class or a path/identifier to a pretrained tokenizer."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99095667-83e2-41a1-9ecd-2a028c6b7394",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
